import os
import json
import logging
import argparse
import subprocess

from typing import Dict, Any
from collections import defaultdict

import ir_datasets
import pytrec_eval
import numpy as np
from tqdm import tqdm
from pyserini.search.lucene import LuceneSearcher

from src import utils
import tot_zh

from jnius import autoclass
import opencc

log = logging.getLogger(__name__)
cc = opencc.OpenCC('t2s.json')

METRICS = (
    "recall_10,recall_100,recall_1000,ndcg_cut_10,ndcg_cut_100,ndcg_cut_1000,recip_rank"
)


def create_index(dataset, field_to_index, dest_folder, index):
    log.info(f"creating files for indexing in {dest_folder}")
    docs_folder = os.path.join(dest_folder, "docs")
    os.makedirs(docs_folder, exist_ok=True)

    with open(os.path.join(docs_folder, "docs.jsonl"), "w", encoding="utf-8") as writer:
        for raw_doc in dataset.docs_iter():
            # Get the original text
            original_contents = getattr(raw_doc, field_to_index)
            # Convert it to Simplified Chinese
            simplified_contents = cc.convert(original_contents) if original_contents else None  
            doc = {"id": raw_doc.doc_id, "contents": simplified_contents}
            writer.write(json.dumps(doc, ensure_ascii=False) + "\n")

    # call pyserini indexer
    cmd = f"""python -m pyserini.index.lucene \
      --collection JsonCollection \
      --input {docs_folder} \
      --index {index} \
      --generator DefaultLuceneDocumentGenerator \
      --threads 1 \
      --storePositions --storeDocvectors --storeRaw""".split()

    try:
        subprocess.call(cmd)
    except subprocess.CalledProcessError as e:
        log.exception("Exception occurred during indexing!")
        raise ValueError(e)


def create_run(
    index, queries, param_k1, param_b, batch_size, n_hits, n_threads, param_mu
):
    run = {}
    searcher = LuceneSearcher(index)

    """
    turn on one similarity metric at a time
    """

    # BM25
    # searcher.set_bm25(k1=param_k1, b=param_b)

    # QLD, LMDirichletSimilarity
    searcher.set_qld(param_mu)

    # TF-IDF
    # searcher.object.similarty = autoclass("org.apache.lucene.search.similarities.ClassicSimilarity")()

    # # LMJelinekMercerSimilarity
    # searcher.object.similarty = autoclass('org.apache.lucene.search.similarities.LMJelinekMercerSimilarity')(0.7)

    # # IBSimilarity
    # Distribution = autoclass('org.apache.lucene.search.similarities.DistributionLL') # Log-logistic distribution
    # Lambda = autoclass('org.apache.lucene.search.similarities.LambdaTTF')  # total term frequency
    # Normalization = autoclass('org.apache.lucene.search.similarities.NormalizationH3')
    # searcher.object.similarty = autoclass('org.apache.lucene.search.similarities.IBSimilarity')(Distribution(), Lambda(), Normalization())

    # # DFRSimilarity
    # BasicModelG = autoclass('org.apache.lucene.search.similarities.BasicModelG')
    # AfterEffectL = autoclass('org.apache.lucene.search.similarities.AfterEffectL')
    # NormalizationH1 = autoclass('org.apache.lucene.search.similarities.NormalizationH1')
    # searcher.object.similarty = autoclass('org.apache.lucene.search.similarities.DFRSimilarity')(BasicModelG(), AfterEffectL(), NormalizationH1())

    # # DFISimilarity
    # IndependenceStandardized = autoclass('org.apache.lucene.search.similarities.IndependenceStandardized')
    # searcher.object.similarty = autoclass('org.apache.lucene.search.similarities.DFISimilarity')(IndependenceStandardized())

    # # Axiomatic (AxiomaticF1EXP, AxiomaticF1LOG, AxiomaticF2EXP, AxiomaticF2LOG, AxiomaticF3EXP, AxiomaticF3LOG)
    # searcher.object.similarty = autoclass('org.apache.lucene.search.similarities.AxiomaticF1EXP')

    batches = list(utils.batched(queries, batch_size))
    for batch in tqdm(batches):
        batch_qids, batch_queries = [_[0] for _ in batch], [_[1] for _ in batch]
        simplified_batch_queries = [cc.convert(q) if q else "" for q in batch_queries]
        results = searcher.batch_search(
            simplified_batch_queries, # Use the simplified queries
            batch_qids, 
            k=n_hits, 
            threads=n_threads
        )
        for qid, hits in results.items():
            assert qid not in run
            run[qid] = {}
            for i in range(len(hits)):
                run[qid][hits[i].docid] = hits[i].score
    return run


if __name__ == "__main__":
    parser = argparse.ArgumentParser("BM25 Run")
    parser.add_argument("--data_path", required=True, help="location to dataset")
    parser.add_argument(
        "--split", required=True, choices={"train", "dev", "test"}, help="split to run"
    )
    parser.add_argument("--field", required=True, help="field to index from documents")
    parser.add_argument("--index_name", required=True, help="name of index")
    parser.add_argument(
        "--query", choices=["title", "text", "title_text"], required=True
    )

    parser.add_argument(
        "--param_k1", default=0.8, type=float, help="param: k1 for BM25"
    )
    parser.add_argument("--param_b", default=1.0, type=float, help="param: b for BM25")
    parser.add_argument(
        "--param_mu", default=1000, type=float, help="param: mu for QLD"
    )

    parser.add_argument("--run", default=None, help="(optional) path to save run")
    parser.add_argument(
        "--run_format",
        default=None,
        choices={"trec_eval", "json"},
        help="(optional) path to save run, defaults to json if json is in file name",
    )
    parser.add_argument(
        "--run_id", default=None, help="run id (required if run_format = trec_eval)"
    )

    parser.add_argument(
        "--metrics", required=False, default=METRICS, help="csv - metrics to evaluate"
    )
    parser.add_argument(
        "--docs_path",
        default="./anserini_docs",
        help="path to store (temp) documents for indexing",
    )
    parser.add_argument(
        "--index_path", default="./anserini_indices", help="path to store (all) indices"
    )
    parser.add_argument(
        "--n_hits", default=1000, type=int, help="number of hits to retrieve"
    )
    parser.add_argument(
        "--n_threads", default=8, type=int, help="number of threads (eval)"
    )
    parser.add_argument("--batch_size", default=16, type=int, help="batch size (eval) ")
    parser.add_argument(
        "--negatives_out",
        default=None,
        help="if provided, dumps negatives for use in training other models",
    )
    parser.add_argument(
        "--n_negatives", default=10, type=int, help="number of negatives to obtain"
    )

    logging.basicConfig(level=logging.INFO)

    args = parser.parse_args()
    tot_zh.register(args.data_path)
    split = args.split

    irds_name = "zh-corpus:" + split
    dataset = ir_datasets.load(irds_name)

    metrics = args.metrics.split(",")

    log.info(f"metrics: {metrics}")

    docs_path = os.path.join(args.docs_path, args.index_name)
    index = os.path.join(args.index_path, args.index_name)
    if os.path.exists(index):
        log.warning(f"Index {index} already exists!")
    else:
        log.info("Creating index!")
        create_index(
            dataset=dataset,
            field_to_index=args.field,
            dest_folder=docs_path,
            index=index,
        )

    # log.info(f"BM25 config: k1={args.param_k1}; b={args.param_b}")
    log.info(f"LMD config: mu={args.param_mu}")

    queries, n_empty = utils.create_queries(dataset, query_type=args.query)

    # [ADDED] Map QIDs to domains
    # We assume the query object has a 'domain' attribute. 
    # If your dataset uses a different attribute name (e.g., 'category'), change 'domain' below.
    qid_to_domain = {}
    for q in dataset.queries_iter():
        qid_to_domain[q.query_id] = getattr(q, 'domain', 'unknown')

    log.info(f"Gathered {len(queries)} queries")
    if n_empty > 0:
        log.warning(f"Number of empty queries: {n_empty}")

    run = create_run(
        index=index,
        queries=queries,
        param_b=args.param_b,
        param_k1=args.param_k1,
        batch_size=args.batch_size,
        n_hits=args.n_hits,
        n_threads=args.n_threads,
        param_mu=args.param_mu,
    )

    # [ADDED] Initialize variable to store domain-specific aggregated results
    domain_agg_results = {}

    if dataset.has_qrels():
        qrel, n_missing = utils.get_qrel(dataset, run)
        if n_missing > 0:
            raise ValueError(f"Number of missing qids in run: {n_missing}")

        evaluator = pytrec_eval.RelevanceEvaluator(qrel, metrics)

        eval_res = evaluator.evaluate(run)

        # [ADDED] Calculate and log aggregated results per domain
        domain_metric_values = defaultdict(lambda: defaultdict(list))
        
        # Group scores by domain
        for qid, q_metrics in eval_res.items():
            domain = qid_to_domain.get(qid, "unknown")
            for metric, value in q_metrics.items():
                domain_metric_values[domain][metric].append(value)

        # Compute mean and std for each domain
        log.info("=== Domain-specific Results ===")
        for domain, metric_dict in domain_metric_values.items():
            domain_agg_results[domain] = {}
            log.info(f"--- Domain: {domain} ---")
            for metric, values in metric_dict.items():
                mean = np.mean(values)
                std = np.std(values)
                domain_agg_results[domain][metric] = (mean, std)
                log.info(f"{metric:<12}: {mean:.4f} ({std:0.4f})")
        log.info("===============================")

        # Original Overall Aggregation
        eval_res_agg = utils.aggregate_pytrec(eval_res, "mean")

        log.info("=== Overall Results ===")
        for metric, (mean, std) in eval_res_agg.items():
            log.info(f"{metric:<12}: {mean:.4f} ({std:0.4f})")

    else:
        log.info("dataset does not have qrels. evaluation not performed!")
        eval_res_agg = None
        eval_res = None
        qrel = None

    if args.run is not None:
        run_format = args.run_format
        if run_format is None:
            run_format = "json" if "json" in args.run else "trec_eval"
        log.info(f"Saving run to {args.run} (format={run_format})")
        if run_format == "json":
            utils.write_json(
                {
                    "aggregated_result": eval_res_agg,
                    "domain_aggregated_result": domain_agg_results, # [ADDED] Save domain results to JSON
                    "run": run,
                    "result": eval_res,
                    "args": vars(args),
                },
                args.run,
                zipped=args.run.endswith(".gz"),
            )
        else:
            run_id = args.run_id
            assert run_id is not None
            with open(args.run, "w") as writer:
                for qid, r in run.items():
                    for rank, (doc_id, score) in enumerate(
                        sorted(r.items(), key=lambda _: -_[1])
                    ):
                        writer.write(
                            f"{qid}\tQ0\t{doc_id}\t{rank}\t{score}\t{run_id}\n"
                        )

        if args.negatives_out:
            out = {}
            for qid, hits in run.items():
                hits = sorted(hits.items(), key=lambda _: -_[1])
                negs = []
                for (doc, score) in hits:
                    if qrel[qid].get(doc, 0) > 0:
                        continue
                    if len(negs) == args.n_negatives:
                        break
                    negs.append(doc)

                out[qid] = negs

            utils.write_json(out, args.negatives_out)